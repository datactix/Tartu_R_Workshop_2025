---
title: "PCA and clusterisation"
subtitle: "A complete analysis case"
format:
  html:
    toc: true
    toc-location: left
    toc_float: true
    embed-resources: true
    theme: cosmo
    reference-location: margin
    citation-location: margin
    grid:
      sidebar-width: 300px
      body-width: 900px
      margin-width: 300px
editor: 
  markdown: 
    wrap: sentence
---

```{r general_setup, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Import the data and load the libraries

To help reproducibility :
```{r}
set.seed(123)
```


We load the libraries we are using :

```{r load_lib}
library(tidyverse)
library(here)
library(readr)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggpubr)
library(plotly)
library(cowplot)
library(kableExtra)
```

We load the datasets :

The data are in the `.tsv` format (**t**ab **s**eparated **v**alues), to load them we will use the `readr` dedicated function [`read_tsv()`](https://readr.tidyverse.org/reference/read_delim.html)

```{r, results=FALSE}

meta_data <- readr::read_tsv(here::here('data/curated_datasets/MI_eCRF_Curated.tsv')) 

# We extract the nanostring data 
ns_data <- readr::read_tsv(here::here('data/curated_datasets/MI_Nanostring-eCRF_Curated.tsv')) %>% 
  dplyr::select(- dplyr::all_of(colnames(meta_data) %>% setdiff("SUBJID"))) %>% tidyr::drop_na()

```

<details>

<summary>**BONUS** Check the data (never trust the data) : the nanostring dataset</summary>

\*\* This part is a bit out of the scope of this lesson, but to illustrate what should be real data analysis plan, we chose to present it anyway \*\*

Data generation is a long process, eventually involving multiple peoples, technologies and centers over long period of time.
This complexity strongly increase the risk of errors (missing data, swapped samples, etc ...) and loss of quality (batch effect, inconsistency).

A good practice is to systematically check for the integrity and quality of the data before starting any project.

In our case, a good start would be to check some general facts about our dataset :

-   How many samples do we have in our dataset ?

```{r}
ns_data %>% nrow()
```

-   How many donors do we have in our dataset ?

```{r}
ns_data %>% dplyr::pull('SUBJID') %>% unique() %>% length()
```

-   How many and which stimulation do we have in our dataset ?

```{r}
ns_data %>% dplyr::pull('Stimulus') %>% unique()
```

-   Do we have all the stimulation condition for each donors ?

```{r}
ns_data %>% 
  dplyr::select('SUBJID', 'Stimulus') %>% 
  dplyr::group_by(Stimulus) %>%
  dplyr::summarise_all(~ dplyr::n_distinct(SUBJID))
```

-   Do we have only one value for each donors \* stimulus (check the absence of duplicates) ?

Taking into account the results to the previous questions we can already assert that we don't have duplicates, but for pedagogical purposes here is a way to extract all the duplicated couple of donors \* stimulus :

```{r check_duplicates}

ns_data %>% 
  dplyr::group_by(SUBJID, Stimulus) %>%
  dplyr::summarise(nb = n()) %>%
  dplyr::ungroup() %>% 
  dplyr::filter(nb > 1)

```

-   Do we have missing values, and if yes, where ?

First check column wise :

```{r check_na_column_wise}
ns_data %>%
  dplyr::summarise_all(~sum(is.na(.))) %>%
  tidyr::pivot_longer(cols = dplyr::where(is.numeric) ,names_to = "variable", values_to = "na_count") %>% 
  dplyr::filter(na_count > 1)
```

Second check row wise, by SUBJID :

```{r check_na_row_wise}
ns_data %>%
  dplyr::group_by(SUBJID) %>% 
  dplyr::summarise_all(~sum(is.na(.))) %>% 
  # row sum will sum all the column for each row, we remove the SUBJID to avoid getting him summed with the na count of the other columns 
  dplyr::filter(rowSums(dplyr::select(., -SUBJID)) > 0)

```

\*\* $\rightarrow$ OK !
Our dataset seems complete.\*\*

</details>

<details>

<summary>**BONUS** Check the data (never trust the data) : the meta-data dataset</summary>

As we will use them later we can continue by checking the meta-data and their consistency with the nanostring dataset :

-   We check the number of donors and their consistency between dataset :

```{r}
meta_data %>% dplyr::pull('SUBJID') %>% unique() %>% length()
```

That's more donors than we have in our nanostring dataset.
Let's check if we have data from all the donors presents in the nanostring dataset.
To do so we can extract all the donor id from the nanostring dataset that are not present in the meta_data dataset :

```{r}
setdiff(dplyr::pull(ns_data, 'SUBJID'), dplyr::pull(meta_data, 'SUBJID')) %>% length()
```

Ok, we are not missing any donors.

-   We check that for all the donors in the nanostring dataset we have the complete meta-data :

```{r check_na_metadata}

meta_data %>%
  # we first extract all the donors from the nanostring dataset 
  dplyr::filter(SUBJID %in% unique(dplyr::pull(ns_data, SUBJID))) %>%
  # we can apply again the code we wrote previsouly
  dplyr::group_by(SUBJID) %>% 
  dplyr::summarise_all(~sum(is.na(.))) %>% 
  # row sum will sum all the column for each row, we remove the SUBJID to avoid getting him summed with the na count of the other columns 
  dplyr::filter(rowSums(dplyr::select(., -SUBJID)) > 0) 

```

OK, we are ot missing any information about any of our donors.

\*\* $\rightarrow$ Our meta_data seems complete.\*\*

</details>

# A first overview the data

As a preliminary analysis we will try to get a global overview of our dataset.
Given the high dimensionality of our dataset, the PCA is the perfect tool for this.
It will allow us to quickly get a general understanding of the way our data are structured.
This first knowledge will be the ground for new hypothesis, will help refining existing ones.

## Our first PCA object

To perform the PCA we will use the [FactoMineR library](http://factominer.free.fr/index.html), a turnkey solution that allow you to perform various dimensionality reduction analysis.
Other implementation of PCA exist in R, such as the [prcomp function](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp) from the base R package stats.
This function is more flexible but require more manual work to extract the information.

The first step is to produce the pca object itself.
This object will contain all the information about the PCA.

Let's start by reading the [documentation](https://rdrr.io/cran/FactoMineR/man/PCA.html) of this function :

![Screenshot of the documentation of the `PCA()` function from the `FactoMineR`package. Full webpage can be accessed here : https://rdrr.io/cran/FactoMineR/man/PCA.html](img/doc_pca.png){fig-align="center" width="80%"}

### Prepare the data for the PCA

#### 1. Dataset structure :

The documentation of the PCA function states that `X`, the dataframe object containing our data, should be oriented in such a way that our observations (in our case the samples) are in rows and the quantified parameters (in our case the gene expression level) in columns.

$\rightarrow$ We are lucky, our dataset is already organized in such way.

#### 2. Keep only the numerical variables (corresponding to measured parameters)

The documentation also mention that our data should contains only numerical values (PCA cannot handle non numerical values).
That is a bit more annoying : our dataset also contains some extra non numerical information (like the stimulus) as well as numerical information that are not measured parameters (like the subject id).

We remove them to keep only the measured parameters (gene transcripts) :

```{r, results=FALSE}
data_for_pca <- ns_data %>%
   # we remove any columns that is not a nanostring measure
   dplyr::select(-dplyr::any_of(c('SUBJID','Stimulus','batch.NS','batch.TruCult'))) 
  
```

#### 3. Transform your data

PCA are sensitive to the distribution and scale of our data, thus, we need to be particularly cautious while choosing the transformation method.
In the ideal textbook case (relatively homogeneous data) the most common strategy is to scale the data.

In our case we are dealing with transcriptional data, we want to down-weight the high value, reducing the dynamic range without over-weighting weakly expressed genes (low values).
A possible solution, widely used, is to log transform the gene expression levels however the log transformation does not apply well to values under 1, and our dataset seems contains quite a few of them :

```{r, results = FALSE}
# in order to apply a log transformation we should first check that we don't have values of zero and ideally no values under 1
data_for_pca %>%
          dplyr::summarise(dplyr::across(dplyr::where(is.numeric),~sum(. < 1, na.rm = TRUE))) %>%
          tidyr::pivot_longer(cols = dplyr::where(is.numeric) ,names_to = "variable", values_to = "count_values_under_1") %>%
  dplyr::filter(count_values_under_1 > 0)

  
```

A common workaround is to transform using the function $f(x) \rightarrow log(x+1)$ instead.

A better approach is to use the arc sinus hyperbolic ($arsinh(x) \rightarrow log(x+\sqrt{x^2 + 1})$) this transformation will behave similarly to the logarithm for high values while handling smoothly values in the range of $[0;1]$.

Let's do this :

```{r, results=FALSE}
# the function to calcul the arc sius hyperbolic in r is asinh, we apply it to all our data
data_for_pca <- data_for_pca %>%
         dplyr::mutate( dplyr::across(dplyr::everything(), asinh))
```

### Compute the PCA

We can now apply the \`PCA\` function to our data :

```{r, results = FALSE}

pca_object <- data_for_pca %>%
        FactoMineR::PCA(graph = FALSE, ncp = 10)

```

Note the parameters we used :

-   \`graph = F\` by default the function produce a certain number of basic visualization, we turn this option to false : we don't need them, we will generate them independently later (and of a better quality)

The PCA object we generate this way contains all the principal component as well as the projection of each observation

## Visualization : a first overview on PC 1 and 2

Now that we have our PCA object, we can start to explore it !

PCA are extremely useful to reveals the underlying organisation of our data[^1].

[^1]: By "organization" we means the set of correlations between variables or observation, if any, that structure a dataset.

The most informative way to visualize a PCA is the individuals projection on a scatter plot, to do so we will use the `factoextra` library.
This library will offer us a complete toolkit to explore a PCA object.

One of the function we will use the most is the [`fviz_pca_ind()`](https://search.r-project.org/CRAN/refmans/factoextra/html/fviz_pca.html) this a versatile function that will allow us to explore the individuals (the observation) projection in our PCA.

```{r}
# we set label to "none" to avoid having a cloud of overlapping id, making the plot unreadable
pca_object %>% factoextra::fviz_pca_ind( axes = c(1,2), label = "none")
```

We can clearly observe 3 clusters, an obvious supposition would be that they are corresponding to the stimulus in our dataset.
To check this we can color our plot by stimulus, this is done using the `habillage` parameter of `fviz_pca_ind()` :

```{r}
pca_object %>% factoextra::fviz_pca_ind( label = "none", habillage = as.factor(ns_data$Stimulus))
```

This PCA show a really good clusterisation for the *Null* (no stimulation) and *IAV* (stimulation by *influenza* virus), the other stimulation however tends to group all together.

We will have to explore the next components to see if we can better discriminates these groups.

### Select the number of component to consider

A PCA is not limited to its first two components, the next ones can also retains some interesting signal.
To explore the PCA components we first needs to know which percentage of the total variance of the dataset they explain.
This percentage of total variance explained (also known as "eigenvalue") by component can be summarized using a barplot, this particular plot usually named "screeplot" can be easily generated using the `factoextra` library using the [`fviz_screeplot()`](https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/eigenvalue) function.

```{r}
pca_object %>% factoextra::fviz_screeplot()
```

There is no clear rules to choose how many component you could consider.
One guideline you can apply is too keep all the components with an eigenvalue over 1 (with the logic that an eigenvalue over 1 still explains more variance that an original variable considered alone).
Another approach is to keep all the component until you explain a certain percentage of the total variance.
In our case, to keep this lesson easily readable (and avoid over-crowded figure) we will limit ourselves to the first 5 five components.

### Visualize all your component of interest at once :

Rather than to going through all the pair of components one by one, we can visualize all at once using a grid plot of the different components combination :

```{r, fig.height = 15, fig.width = 15}
# the number of component to consider
n_compo <- 5

#first we generate all the combination
combination_component <- tidyr::crossing(compo_x = 1:n_compo , compo_y = 1:n_compo)


#the function we will use to generate each plot (scatter plot projection of the PCA individuals), given the component for each axes x and y
plot_my_pca <- function(pca_obj, .x, .y){
  # we will use factoextra to generate each sub-plot, to change the properties (ie using the bi-plot) you just have to moify the following line :
  factoextra::fviz_pca_ind(pca_obj,axes = c(.y,.x), label = "none", habillage = as.factor(ns_data$Stimulus)) +
  # the titles of each sublplot wouold take too much space, we take advantage of ggpplot2 format of the 
  # plot generated by factoextra to get rid of them using the dedcated ggplot2 library
  ggplot2::ggtitle('')
}

# purrr will allow us to submit directly the given component to the function
# allowing us to avoid the writing of a complex loop. It will return a list containing the results of the
# calls of this functions for each combination of x and y 
purrr::map2(.x = combination_component$compo_x,
            .y = combination_component$compo_y,
            .f = ~ plot_my_pca(pca_object, .x, .y)) %>%
  # ggarrange will allow us to organize all of these subplot into one unique plot
  ggpubr::ggarrange(plotlist = ., ncol = n_compo, nrow = n_compo, common.legend = TRUE, legend = "bottom")


```

Here is a summary of the component discrimination power, observed by eyes, between the different stimulus :

-   component 1 : good discrimination between the *NS*, *IAV* and the other stimulus

-   component 2 : light discrimination of each stimulation at the exception of *E.Coli*, *BCG* and *S.Aureus* 

-   component 3 : light discrimination of *C.Albicans*

-   component 4 : almost no clear discrimination between any of the stimulus can be observed

-   component 5 : good discrimination of *SEB* and light discrimination of all the other stimulus


To sum up all the information we got from this visualization, we can say that this first 5 components capture the signal allowing to discriminate between *NS*, *IAV*, *C.ALbicans*, *SEB* and a cluster constituted by all the bacterial stimulus (*E.Coli*, *BCG* and *S.Aureus*).
The distance between the cluster, and the mixing of different stimulation in one cluster is also informative,  it tells us that that some stimulus are more similar than others. 
For example, we can conclude from the proximity of the *BCG*, *E.Coli* and *S.Aureus* stimulus that they elicit a relatively similar immune response.
Finally an other interesting point, is the shape of the cluster formed by a given stimulus. The spread of these cluster express the inter-individual variability in the response to a given stimulus.  It's particularly visible for the *IAV* (*Influenza*) stimulus. Some donors are projected between the *IAV* cluster and the *NS* cluster (it is tempting to interpret them as individuals exhibiting a weak anti-viral response against *Influenza*)


## (advanced user) Bonus : visualize 3 components at once using an interactive 3D plot

It happens that we ended up interested in 3 components, as solution to visualize them is to create the 3 scatter plot (one for each possible pair of variables), but a more straightforward solution is often to draw a 3D scatter plot.
The `ggplot2` library we used till there do not support interactive 3D, to perform this task we will rather use the `plotly` library :

```{r}

# 1 - we extract the projected values in the 3 first component :
# To do so we will have to manually dig into the pca_object : a complicated structure made of nested list of matrix
pca_object$'ind'$'coord' %>%
  # first lets turn it into a beautiful tibble (the format of data_frame tidyverse is using)
  # tibble does not have rownames, but we can ask him to convert the rownames into a proper column using the rownames argument
  tibble::as_tibble(rownames = 'sample_id') %>%
  # we select the 3 first components
  dplyr::select(dplyr::all_of(c('sample_id', paste0('Dim.',1:3)))) %>%
  # we submit them to plotly
  plotly::plot_ly(x = ~Dim.1, y = ~Dim.2, z = ~Dim.3, text = ~sample_id, color = ns_data$Stimulus, trace = 'scatter3d')

```



This kind of visualization is particularly helpful during the first exploratory phase of analysis, the interactivity offered by `plotly` can save a lot of time.
However the environment around the `plotly` library is poorer than ggplot2 (especially for bioinformatic visualization) : producing a mature plot may require more time and efforts.

<details>

<summary>**BONUS** : Never trust the data (episode 2) : hunt the technical variability</summary>

Because the clusterizations pattern we observe on the PCA are consistent with the underlying biology we could easily suppose that all this nice clusters are a pure reflection of the various immune pathway differentialy activated in response to each stimulation.
But such kind of large experiments is highly susceptible to include some part of technical variability, one of the most important factor to check is the batches : our dataset includes `r nrow(data_for_pca)` samples.
This high number makes the analysis in one run impossible, we probably have diverse runs (of samples collections, preparation, analysis, reading) eventually using diverse batch of reagent.
Knowing how the experiment was done and keeping a trace of all this step is essential while analyzing the data.

In our case the experiments was done in 3 batches, we can check how the stimulus are balanced between batches :

```{r, eval = FALSE}
ns_data %>%
    group_by(batch.NS, Stimulus) %>%
    summarise(n_samples = n())
  
```

Or even better visualize it :

```{r, eval = FALSE}

ns_data %>%
    ggplot(aes(x = batch.NS, fill = Stimulus)) +
    geom_bar() +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Number of samples per run", x = "Run", y = "Number of samples")
  

```

We have a problem for *C.albicans*, *IAV* and *SEB* : this stimulations are only included in the batch 1.
We could be worried that these stimulation form distinct cluster because of some technical variation between run rather than biological variability.

This lack of randomisation is common weakness in a lot of experiments design, either by mistake or because of logistical constraints.
Even if methods exists to deal with this issues (in most cases), the best is to minimize the technical variability (or, if not possible, to prepare control strategy to estimate/correct for it) as much as possible at the experiment design step.

The PCA is an easy way to have a first check on the impact of this putative batch effect.
We can start by coloring our previous mosaic PCA plot using the batches informations rather than the stimulation :

```{r, fig.height = 20, fig.width = 20, eval = FALSE}
# the number of component to consider
n_compo <- 5

#first we generate all the combination
combination_component <- tidyr::crossing(compo_x = 1:n_compo , compo_y = 1:n_compo)

plot_my_pca <- function(pca_obj, .x, .y){factoextra::fviz_pca_ind(pca_obj,axes = c(.y,.x), label = "none", habillage = as.factor(ns_data$batch.NS)) + ggplot2::ggtitle('')}

purrr::map2(.x = combination_component$compo_x,
            .y = combination_component$compo_y,
            .f = ~ plot_my_pca(pca_object, .x, .y)) %>%
  ggpubr::ggarrange(plotlist = ., ncol = n_compo, nrow = n_compo, common.legend = TRUE, legend = "bottom")


```

We don't see any cluster with a clear separation induced by the batch, but this PCA is probably not the best choice to check for this.
We can rather focus on stimulation that were well balanced between the batches.
A good pick would be to check *NS* and *E.Coli*, we can expect to have genes whith lower level of expression for the *NS* and others, responding to *E.Coli* presenting high level of expression.

```{r, eval = FALSE}

# We start from the raw data, in order to keep consistency between our data and the Stimulus / Batch columns
# we filter the data to keep only the stimulus we are interested in :
null_ecoli_data <- ns_data %>%
  dplyr::filter(Stimulus %in% c('Null', 'E.coli'))

null_ecoli_pca <- null_ecoli_data %>%
  dplyr::select(-dplyr::all_of(c('SUBJID','Stimulus','batch.NS','batch.TruCult'))) %>%
  dplyr::mutate_all(.funs = asinh) %>%
  FactoMineR::PCA(graph = FALSE, scale.unit = FALSE, ncp = 10)


```

::: panel-tabset
## Component 1 / 2

```{r, eval = FALSE}

plot_null_ecoli_pca_col_stim <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(1,2),  label = "none", habillage = as.factor(null_ecoli_data$Stimulus))

plot_null_ecoli_pca_col_batch <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(1,2), label = "none", habillage = as.factor(null_ecoli_data$batch.NS))

ggpubr::ggarrange(plotlist = list(plot_null_ecoli_pca_col_stim, plot_null_ecoli_pca_col_batch), ncol = 2, nrow = 1, legend = "bottom")
```

## Component 3 / 4

```{r, eval = FALSE}

plot_null_ecoli_pca_col_stim <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(3,4),  label = "none", habillage = as.factor(null_ecoli_data$Stimulus))

plot_null_ecoli_pca_col_batch <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(3,4), label = "none", habillage = as.factor(null_ecoli_data$batch.NS))

ggpubr::ggarrange(plotlist = list(plot_null_ecoli_pca_col_stim, plot_null_ecoli_pca_col_batch), ncol = 2, nrow = 1, legend = "bottom")
```

## Component 5 / 6

```{r, eval = FALSE}

plot_null_ecoli_pca_col_stim <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(5,6),  label = "none", habillage = as.factor(null_ecoli_data$Stimulus))

plot_null_ecoli_pca_col_batch <- null_ecoli_pca %>%
  factoextra::fviz_pca_ind(axes = c(5,6), label = "none", habillage = as.factor(null_ecoli_data$batch.NS))

ggpubr::ggarrange(plotlist = list(plot_null_ecoli_pca_col_stim, plot_null_ecoli_pca_col_batch), ncol = 2, nrow = 1, legend = "bottom")
```
:::



We did not capture any visible batch effects on any of the components\[\^longnote_2\].
To be sure of the absence of batch effects more in depth analysis is needed, but the statistical tools to use for this are not in the scope of the today's lesson.

\[\^longnote_2\] This dataset is actually not the raw dataset, some pre-treatment was already done on it to correct the batch effects.
This first quality control / data integration step can be extremely time and effort consuming (keep this in mind while planning your project timeline).

</details>

# A step further : understand what drives this variability

::: callout-tip
## Let's summarize what we have done

After checking its consistency we have performed a PCA on our dataset in order to get a first overview from it.
Doing so we have done a certain number of observation :

-   we have observed its organisation into distinct clusters
-   these cluster are corresponding to different group of stimulus
-   these cluster are not induced by technical source of variability (bonus part)
-   the clusters are not homogeneous indicating an inter-individual variability in the response to these stimulation
:::

To better understand these clusters, (*ie* the response to the stimulus included represented in these cluster) we need now to explore the way the different component are discriminating between them (*ie* which genes are selected by the component to discriminate the cluster we observe).

**To rephrase it in a more technical point of view we need to switch our attention from the individuals (the observations : the samples) to the variable (the parameters : the gene expression levels).**

## Extract information from the PCA components

It is noticeably the case of the component 1 and 2, that are discriminating the *Null*, *IAV* and a "blob" constituted by the other stimulus.
A complete analysis would have to scrutinize all the component capturing some interesting signal (in our case at least all the first 4 component).

In this lesson for time matter, we will only focus on the two first components : we will extract the genes selected by their loadings that allow this discrimination.

### Use and interpret biplots vizualisation

A solution could be to manually dig into the pca object, but `FactoMineR`and `factoextra` provide some nice functions that will avoid us this complicated manipulation.

One of this function is [`fviz_pca_var()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_pca.html), the counterpart of `fviz_pca_ind()`, but tailored for exploration of the "variables" rather than the "individuals" in our PCA.

In our case we are particularly interested on four arguments :

-   X : the pca object
-   axes : the couple of components we want to consider
-   col.var : the column we will use for the coloration of our graph
-   select.var : a condition to select the variables to display, for exemple list('contrib' = 10) will pick the 10 top contributors
-   repel : when we try to visualize multiple at a time the risk that the graph get unreadable because of the overlap between different label is high, to avoid this we can use this option that will arrange the label automatically to keep the graph readable.


```{r}
pca_object %>% factoextra::fviz_pca_var(col.var="contrib", select.var = list('contrib' = 25),  repel = TRUE)
```

How to read this plot :

*   The coordinate of the vectors are his loadings in each component (it indicates us what drives the distribution of the individuals acording to each pair of components)
*   The closer (same direction, or opposite direction) and the more similar : the more correlated (for exemple `CXCL11` and  `TNFSF10` seems to be correlated).


We can now enrich our plot with the projection of the individuals, this kind of plot is known as a "biplot".
We will do this using the function [`fviz_pca_biplot()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_pca.html)

```{r}
pca_object %>% factoextra::fviz_pca_biplot( select.var = list(contrib = 25),  habillage = as.factor(ns_data$Stimulus), label = 'var')
```

Superimposing both the variables and the observations projections, give us some nice hint about our data :

* The genes that drives PC 1 and 2 are up-regulated in response to a stimulus : their vectors point towards the NS cluster opposite direction

*   Our IAV cluster is driven by as subset of genes (including `TNFSF10`, `CXCL11` and `TNFSF13B`) that are specific to the *IAV* stimulus (their loading in the PC1 that capture the other stimulus is close to 0)


## Visualize loading for only one component

We can be interested by only one components, `factoextra` provide the [`fviz_contrib()`](https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_contrib) function that allow us to visualize them as a barplot :  


```{r}
# choice = "var" indicates that we want to see the most contributing variable of a given component
# axes = 1   allow us to sepcify that this given component is the component 1
plot_contrib_pc1 <- factoextra::fviz_contrib(pca_object, choice = 'var', axes = 1, top = 25) +
  ggplot2::ggtitle('Top contributors of the PC1')

plot_contrib_pc2 <- factoextra::fviz_contrib(pca_object, choice = 'var', axes = 2, top = 25) +
  ggplot2::ggtitle('Top contributors of the PC2')

ggpubr::ggarrange(plotlist = list(plot_contrib_pc1, plot_contrib_pc2), ncol = 1, nrow = 2)
```



### **Bonus** : use the cos2 to better understand what is the signal of given component

As we saw previously TNFSFS10, TNFSFS13B, CXCL11 have high weight in the loading of the PC2.
But can we say that these genes are only strongly contributing to these particular component ?
A good way to do this is to use the cos2 value, this value represent the percentage of a variable variance explained by a given component (the sum of the cos2 values is always equal to 1 for each variables).

```{r}
# PCA objects are complicated object : they are constituted by nestd lists of matrix,
# here is the way to extract the contribution of the  variables to each components :
cos2_pc2 <- pca_object$'var'$'cos2' %>%
    # first lets turn it into a beautiful tibble (the format of data_frame tidyverse is using)
  # tibble does not have rownames, but we can ask him to convert the rownames into a proper column using the rownames argument
  tibble::as_tibble(rownames = 'variable') %>%
  # we select the top 5 components
  dplyr::select(dplyr::all_of(c('variable', paste0('Dim.',1:5)))) %>%
  # we extract the variables of interest
  dplyr::filter(variable %in% c('TNFSF10', 'TNFSF13B', 'CXCL11'))  

```

We can now plot them, a nice way to do this would be to use a correlation plot as we are discussing the link between two "variables", creating a correlation plot from scratch using `ggplot2` can be a bit complicated, but as often with R there is already a library. 
In our case, we can use [`corrplot`](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) to do this.

```{r}

cos2_pc2 %>%
  # corplot only accept matrix with rownames : we reshappe our dataset
  tibble::column_to_rownames('variable') %>%
  as.matrix() %>%
  corrplot::corrplot(is.corr=FALSE,  addCoef.col = "black")

```

The cos2 of our variables of interest are close to 0.85 : a high value, depicting their strong relation with the component 2.
$\rightarrow$ We can say that the component 2 is reflecting the influence of this group of variables.

### **Bonus** : extract a component top contributors as a list

For some kind of analysis (GO enrichment analysis for exemple), you might be interested to extract the name and contribution of all the variable for a given component.
To achieve this so we will have to do a bit of manual digging into the pca_object :

```{r}

# PCA objects are complicated object : they are constituted by nestd lists of matrix,
# here is the way to extract the contribution of the  variables to each components :
loadings_pc1 <- pca_object$'var'$'contrib' %>%
  # first lets turn it into a beautiful tibble (the format of data_frame tidyverse is using)
  # tibble does not have rownames, but we can ask him to convert the rownames into a proper column using the rownames argument
  tibble::as_tibble(rownames = 'variable') %>%
  #we select the component 1
  dplyr::select('Dim.1') %>%
  # we rename to make sense
  dplyr::rename(contrib = 'Dim.1') %>%
  # Sort by 'contrib' column in descending order
  dplyr::arrange(dplyr::desc('contrib')) %>%  
  # Select the top 30 rows
  dplyr::slice_head(n = 30) 
  

# same for PC2 :
loadings_pc2 <- pca_object$'var'$'contrib' %>%
  tibble::as_tibble(rownames = 'variable') %>%
  dplyr::select('Dim.2') %>%
  dplyr::rename(contrib = 'Dim.2') %>%
  dplyr::arrange(dplyr::desc('contrib')) %>%  
  dplyr::slice_head(n = 30) 

```

## Refine the clustering

As we already saw in our "mosaic" PCA, we can identify a certain number of cluster corresponding to the different stimulation.
According to the components we consider we will be more or less able to distinguish between the different stimulations.

But it's hard to get the global picture, given all our components what are the stimulus that form distinct cluster ?

A good way to answer this question is to apply an unsupervised clustering algorithm, this algorithm will identify for us the group of points that cluster together.

There is many different clustering algorithm, kmeans is far to be the best but offers descent performance and is easy to apply.

### Apply kmeans

#### Step 1 : choose the number of cluster

Kmeans came with one constraint : you have to provide yourself the number of cluster you expect from your data.
To help you to define this number, the package `factoextra` provide the function
[`fviz_nbclust`](https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_nbclust).

There is different methods to define the best number of cluster, we will use the "Elbow method", this method works by computing the average distance of all the points inside a cluster this metric can be use a score to evaluate the quality (the compactness) of a cluster, the mean of all the score of each cluster is thus a way to estimate the quality of a clustering.

After a certain number of cluster the quality of clustering stop signfificantly improoving, forming an "elbow". We will select the number of cluster just before this "elbow". 



```{r}

# from our mosaic PCA we can suppose that only the components 1,2,3 and 5 were pertinent for the stimulus_clusterisation
# we select them :
pca_coordinates <- pca_object$ind$coord %>%
  tibble::as_tibble() %>%
  dplyr::select(paste0('Dim.',c(1:3,5))) 

pca_coordinates %>%
  factoextra::fviz_nbclust(kmeans, method = "wss")

```

We can see that the score is decreasing quickly (meaning that the quality of clusterisation is improving) till 5 after which the quality decrease again.

$\rightarrow$ 5 is the best number of cluster we can extract using kmean

#### Step 2 : Apply kmeans

```{r, results = FALSE}
kmeans_result <- pca_coordinates %>%
  kmeans(centers = 5, nstart = 25)

```

#### Step 3 : visualize the clusters :

```{r}
factoextra::fviz_cluster(kmeans_result, 
                         data = pca_coordinates, 
                         ellipse.type = "convex", 
                         geom = "point") # to show only points (and not the labels) 
```

Our clustering was done on the first 5 components of the PCA, not just the two first component, which explains the overlap of cluster in their projection over the two first components.

#### Step 4 : extract the cluster

We could use the different tricks of visualization (changing the markers shape for each stimulation for example) to see in which extent the cluster obtained by the PCA are consistent with the stimulation.
But a more straightforward solution is directly extract the clustering and to compare it with the stimulus as a table :

```{r}

clusterisation_recap_long <- table(as.factor(kmeans_result$cluster), as.factor(ns_data$Stimulus)) %>%  
  as.data.frame() %>%
  dplyr::mutate(Var1 = paste0('Clust_',Var1)) 

clusterisation_recap_wide <- clusterisation_recap_long %>%
  tidyr::pivot_wider(id_cols = Var1, names_from = Var2, values_from = Freq)

clusterisation_recap_wide %>% head() %>% kableExtra::kable() %>% kableExtra::kable_styling()

```

We can use the `corrplot` package to make it more readable :

```{r}
clusterisation_recap_wide %>% 
  tibble::column_to_rownames('Var1') %>% 
  as.matrix() %>%
  corrplot::corrplot(is.corr = FALSE,  addCoef.col = "black")

```

Our methodology allowed us to create 5 consistent clusters, at the exceptions of the cluster 5 they are all discriminating a specific stimulus.
The cluster 5 is constituted by all the bacterial stimulations.

We also note some misclassified samples, these samples are of particular interest, discussing their specificities (if any) is an important step of any analysis.
We can noticed few interesting fact, and propose hypothesis :

-   7 samples stimulated by *IAV* ended up in the cluster capturing un-stimulated donors : these donors might be 'low responder' exhibiting a weaker response to the *IAV* stimulation (keep in mind also the kinetic aspect these donors can also have a delayed response)

-   An important number of our misclassified samples are samples swapped between the bacterial cluster (cluster 4) and the SEB one (cluster 5).
    As SEB is a staphylococcal toxin we can hypothesize the the S.Aureus stimulation contains trace amount of a similar toxin eliciting a similar response, bringing the two cluster together.

The next logical step is to individually investigate these samples and there associated donors.
To do so we first need to extract these samples, this is done with a bit of dataframe manipulation :

```{r}

# First we need the details of each cluster :
# we start from the raw dataset
clusterisation_detailed <- ns_data %>%
  # we keep only the donor id and the stimulus
  dplyr::select(SUBJID, Stimulus) %>%
  # we associate the results from the clustering
  dplyr::mutate(cluster = kmeans_result$cluster) %>%
  # just to make things a bit clearer
  dplyr::mutate(cluster = paste0('Clust_',cluster))
  
# Now me can extract the misclassified donors
misclassified_donors <- clusterisation_detailed %>%
  # we remove the donors that were well classified in the cluster 1 (candida cluster)
  dplyr::filter(! (cluster == 'Clust_3' & Stimulus == 'C.albicans')) %>%
  # we remove the donors that were well classified in the cluster 2 (SEB cluster)
  dplyr::filter(! (cluster == 'Clust_4' & Stimulus == 'SEB')) %>%  
  # we remove the donors that were well classified in the cluster 3 (flu cluster)
  dplyr::filter(! (cluster == 'Clust_5' & Stimulus == 'IAV')) %>%  
  # we remove the donors that were well classified in the cluster 4 (Null cluster)
  dplyr::filter(! (cluster == 'Clust_2' & Stimulus == 'NS')) %>%              
  # we remove the donors that were well classified in the cluster 1 (bacteria cluster)
  dplyr::filter(! (cluster == 'Clust_1' & Stimulus %in% c('BCG','E.coli','S.aureus')))
  
# We can enrich this dataset with metadata from the donors
misclassified_donors <- misclassified_donors %>%
  dplyr::left_join(meta_data, by = 'SUBJID')
  
misclassified_donors %>% kableExtra::kable() %>% kableExtra::kable_styling()

```

Based on the specificity of each of these donors, we could formulate secondary hypothesis and test them (for example, the association between tobacco exposure and *IAV* low response).

But the hypothesis testing is out of the scope of the today's lesson, you will see this during a dedicated lesson.

# Inter-individual variability

::: callout-tip
## Let's summarize

Till there, we have used the PCA to explore the response to immune stimulation.
Dissecting the PCA components, we were able to identify the group of genes activated in response to different stimulus.

In a second time we went back to the clusterisation refining it using kmeans on the components 1,2,3,5 that we previously identified as of interest to discriminate between the stimulus, doing so we were able to better characterized each cluster.
It gave us also the opportunity to have a first dive into the inter-individual variability in the response to a given stimulus : we identified some extreme cases.
These extreme cases, are donors with an immune response to a given stimulus atypical enough to leads to their classifications with other stimulus.
:::

We want now to go further into the study of the inter-individual variability of the immune response.
As we saw previously the immune stimulation in our dataset have a broad effect on the measured genes, thus most of the variability in the dataset is induced by the stimulations.

Despite this overwhelming effect of the stimulation we have already see some manifestation of this inter-individual variability at the scale each stimulation (the spread of each cluster) and were even able to capture some "extreme" cases (misclassified samples).

## Focus the variability in the Flu response

Remember the global PCA (our first PCA, components 1 and 2) ?
The IAV cluster (while being clearly distinct from the other clusters) was spread in a banana-like shape, which indicate a strong variability in the response to IAV (as defined by the component 1 and 2).
we will interest ourselves to factors behind this variability.

To study it we interest ourselves to these particular stimulation and (at a smaller scale) we will reproduce to step of the general analysis previously done.

First let's compute our PCA on the IAV samples :

```{r}

# we start from scratch : we go back to the raw data
data_pca_IAV_only <- ns_data %>%
  # select only the unstimulated
  dplyr::filter(Stimulus == 'IAV') 

object_pca_IAV_only <-  data_pca_IAV_only %>%
  dplyr::select(-dplyr::any_of(c('SUBJID','Stimulus','batch.NS','batch.TruCult'))) %>%
  dplyr::mutate_all(.funs = asinh) %>%
  # let's keep 10 (default 5) components we will refine this later
  FactoMineR::PCA(graph = FALSE, scale.unit = FALSE, ncp = 10)

```

We can have a first look at the screeplot :

```{r}
object_pca_IAV_only %>% factoextra::fviz_screeplot()
```

The first dimension capture a strong part of the variability compared to the other ones, let's have a look to the first 3 components using the same mosaic PCA we did previously.

```{r}
# the number of component to consider
n_compo <- 3

#first we generate all the combination
combination_component <- tidyr::crossing(compo_x = 1:n_compo , compo_y = 1:n_compo)

plot_my_pca <- function(pca_obj, .x, .y){factoextra::fviz_pca_ind(pca_obj,axes = c(.y,.x), label = "none") + ggplot2::ggtitle('')}

purrr::map2(.x = combination_component$compo_x,
            .y = combination_component$compo_y,
            .f = ~ plot_my_pca(object_pca_IAV_only, .x, .y)) %>%
  ggpubr::ggarrange(plotlist = ., ncol = n_compo, nrow = n_compo, common.legend = TRUE, legend = "bottom")
```

Even if we don't observe any nice clusters, that does not mean that we don't have interesting signal.

Among the biological parameters the sex and the age are obvious candidates to explains some of the differences we observe betwen individuals.
We can visually check if they have an impact in the distribution of our samples projection.
To do so we can color our PCA by different factor :

```{r, fig.width=15, fig.height=15}
# first we need the metadata to color our pca
iav_only_w_metadata <- data_pca_IAV_only %>%
  dplyr::select(SUBJID) %>%
  # this way we are sure that the meta-data follow the same order than the one in the dataste used for the PCA object
  dplyr::left_join(meta_data, by = 'SUBJID') %>%
  dplyr::mutate(dplyr::across(dplyr::any_of(c('SUBJID','Sex','CMV','Smoking','BMI')), as.factor))



# the number of component to consider
n_compo <- 5

#first we generate all the combination
combination_component <- tidyr::crossing(compo_x = 1:n_compo , compo_y = 1:n_compo)

plot_my_pca <- function(pca_obj, .x, .y){factoextra::fviz_pca_ind(pca_obj,axes = c(.y,.x), label = "none", habillage = iav_only_w_metadata$Sex) + ggplot2::ggtitle('')}

purrr::map2(.x = combination_component$compo_x,
            .y = combination_component$compo_y,
            .f = ~ plot_my_pca(object_pca_IAV_only, .x, .y)) %>%
  ggpubr::ggarrange(plotlist = ., ncol = n_compo, nrow = n_compo, common.legend = TRUE, legend = "bottom")

```

The components 5 seems to capture a strong sex effect !

To visualize it a bit better, we will generate a boxplot comparing the score of each individual in the PC3 acording to its sex :

```{r}
# first we extract the coordinate of each individual in the PC3 from our PCA object
score_pc4 <- object_pca_IAV_only$ind$coord[, 5]

# we combine it with our meta-data
score_pc3_with_metadata <- data_pca_IAV_only %>% 
  dplyr::select(SUBJID) %>%
  dplyr::mutate(score_pc3 = score_pc4) %>%
  dplyr::left_join(meta_data, by = 'SUBJID')

# from this dataset we can now generate proper boxplot :
score_pc3_with_metadata %>%
  ggplot2::ggplot(aes(x = Sex, y = score_pc4)) + 
  geom_boxplot() + 
  labs(title = "Boxplot of PC5 Scores by Sex", x = "Sex", y = "PC4 Scores") + 
  theme_minimal()
  
  
```

We can extract the leading genes of this component :

```{r}
factoextra::fviz_contrib(pca_object, choice = 'var', axes = 5, top = 20) +
  ggplot2::ggtitle('Top contributors of the PC5')

```

Next step would be statistically validate the sex effect on the level expression of these genes in response to the Flu stimulation.
But it's outside of the scope of this lesson : we will see this in a dedicated lesson !



